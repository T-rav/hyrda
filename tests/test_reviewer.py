"""Tests for dx/hydra/reviewer.py."""

from __future__ import annotations

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from events import EventType
from models import ReviewVerdict
from reviewer import ReviewRunner
from tests.helpers import ConfigFactory, make_streaming_proc

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _make_runner(config, event_bus):
    return ReviewRunner(config=config, event_bus=event_bus)


# ---------------------------------------------------------------------------
# _build_command
# ---------------------------------------------------------------------------


def test_build_command_uses_review_model_and_budget(config, tmp_path):
    runner = _make_runner(config, None)
    cmd = runner._build_command(tmp_path)

    assert "claude" in cmd
    assert "-p" in cmd
    assert "--model" in cmd
    model_idx = cmd.index("--model")
    assert cmd[model_idx + 1] == config.review_model

    assert "--max-budget-usd" in cmd
    budget_idx = cmd.index("--max-budget-usd")
    assert cmd[budget_idx + 1] == str(config.review_budget_usd)


def test_build_command_omits_budget_when_zero(tmp_path):
    from tests.conftest import ConfigFactory

    cfg = ConfigFactory.create(
        review_budget_usd=0,
        repo_root=tmp_path / "repo",
        worktree_base=tmp_path / "wt",
        state_file=tmp_path / "s.json",
    )
    runner = _make_runner(cfg, None)
    cmd = runner._build_command(tmp_path)
    assert "--max-budget-usd" not in cmd


def test_build_command_does_not_include_cwd(config, tmp_path):
    runner = _make_runner(config, None)
    cmd = runner._build_command(tmp_path)

    assert "--cwd" not in cmd


def test_build_command_includes_output_format(config, tmp_path):
    runner = _make_runner(config, None)
    cmd = runner._build_command(tmp_path)

    assert "--output-format" in cmd
    fmt_idx = cmd.index("--output-format")
    assert cmd[fmt_idx + 1] == "stream-json"


# ---------------------------------------------------------------------------
# _build_review_prompt
# ---------------------------------------------------------------------------


def test_build_review_prompt_includes_pr_number(config, event_bus, pr_info, issue):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_review_prompt(pr_info, issue, "some diff")

    assert f"#{pr_info.number}" in prompt


def test_build_review_prompt_includes_issue_context(config, event_bus, pr_info, issue):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_review_prompt(pr_info, issue, "some diff")

    assert issue.title in prompt
    assert issue.body in prompt
    assert f"#{issue.number}" in prompt


def test_build_review_prompt_includes_diff(config, event_bus, pr_info, issue):
    runner = _make_runner(config, event_bus)
    diff = "diff --git a/foo.py b/foo.py\n+added line"
    prompt = runner._build_review_prompt(pr_info, issue, diff)

    assert diff in prompt


def test_build_review_prompt_includes_review_instructions(
    config, event_bus, pr_info, issue
):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_review_prompt(pr_info, issue, "diff")

    assert "VERDICT" in prompt
    assert "SUMMARY" in prompt
    assert "APPROVE" in prompt
    assert "REQUEST_CHANGES" in prompt


def test_build_review_prompt_includes_ui_criteria_when_diff_has_ui_files(
    config, event_bus, pr_info, issue
):
    runner = _make_runner(config, event_bus)
    diff = (
        "diff --git a/ui/src/components/Foo.jsx b/ui/src/components/Foo.jsx\n"
        "+import React from 'react';\n"
        "+export const Foo = () => <div>Hello</div>;\n"
    )
    prompt = runner._build_review_prompt(pr_info, issue, diff)

    assert "DRY" in prompt
    assert "Responsive" in prompt
    assert "Style consistency" in prompt
    assert "Component reuse" in prompt
    assert "theme.js" in prompt


def test_build_review_prompt_excludes_ui_criteria_when_no_ui_files(
    config, event_bus, pr_info, issue
):
    runner = _make_runner(config, event_bus)
    diff = "diff --git a/reviewer.py b/reviewer.py\n+# backend-only change\n"
    prompt = runner._build_review_prompt(pr_info, issue, diff)

    assert "DRY" not in prompt
    assert "theme.js" not in prompt


# ---------------------------------------------------------------------------
# _parse_verdict
# ---------------------------------------------------------------------------


def test_parse_verdict_approve(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "All looks good.\nVERDICT: APPROVE\nSUMMARY: looks good"
    verdict = runner._parse_verdict(transcript)
    assert verdict == ReviewVerdict.APPROVE


def test_parse_verdict_request_changes(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "Issues found.\nVERDICT: REQUEST_CHANGES\nSUMMARY: needs work"
    verdict = runner._parse_verdict(transcript)
    assert verdict == ReviewVerdict.REQUEST_CHANGES


def test_parse_verdict_comment(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "Minor notes.\nVERDICT: COMMENT\nSUMMARY: minor issues"
    verdict = runner._parse_verdict(transcript)
    assert verdict == ReviewVerdict.COMMENT


def test_parse_verdict_no_verdict_defaults_to_comment(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "This is a review without any verdict line at all."
    verdict = runner._parse_verdict(transcript)
    assert verdict == ReviewVerdict.COMMENT


def test_parse_verdict_case_insensitive(config, event_bus):
    runner = _make_runner(config, event_bus)

    transcript_lower = "verdict: approve\nsummary: lgtm"
    assert runner._parse_verdict(transcript_lower) == ReviewVerdict.APPROVE

    transcript_mixed = "Verdict: Request_Changes\nSummary: needs fixes"
    assert runner._parse_verdict(transcript_mixed) == ReviewVerdict.REQUEST_CHANGES

    transcript_upper = "VERDICT: COMMENT\nSUMMARY: minor"
    assert runner._parse_verdict(transcript_upper) == ReviewVerdict.COMMENT


# ---------------------------------------------------------------------------
# _extract_summary
# ---------------------------------------------------------------------------


def test_extract_summary_with_summary_line(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "Review done.\nVERDICT: APPROVE\nSUMMARY: looks good to me"
    summary = runner._extract_summary(transcript)
    assert summary == "looks good to me"


def test_extract_summary_case_insensitive(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "summary: everything checks out"
    summary = runner._extract_summary(transcript)
    assert summary == "everything checks out"


def test_extract_summary_strips_whitespace(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "SUMMARY:   extra spaces around this   "
    summary = runner._extract_summary(transcript)
    assert summary == "extra spaces around this"


def test_extract_summary_fallback_to_last_line(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "First line.\nSecond line.\nThis is the last line"
    summary = runner._extract_summary(transcript)
    assert summary == "This is the last line"


def test_extract_summary_fallback_ignores_empty_lines(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "First line.\nSecond line.\n\n   \n"
    summary = runner._extract_summary(transcript)
    assert summary == "Second line."


# ---------------------------------------------------------------------------
# review - success path
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_review_success_path(config, event_bus, pr_info, issue, tmp_path):
    runner = _make_runner(config, event_bus)
    transcript = (
        "All checks pass.\nVERDICT: APPROVE\nSUMMARY: Implementation looks good"
    )

    mock_execute = AsyncMock(return_value=transcript)
    mock_has_changes = AsyncMock(return_value=False)

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", mock_execute),
        patch.object(runner, "_has_changes", mock_has_changes),
        patch.object(runner, "_save_transcript"),
    ):
        result = await runner.review(pr_info, issue, tmp_path, "some diff", worker_id=0)

    assert result.pr_number == pr_info.number
    assert result.issue_number == issue.number
    assert result.verdict == ReviewVerdict.APPROVE
    assert result.summary == "Implementation looks good"
    assert result.transcript == transcript
    assert result.fixes_made is False


@pytest.mark.asyncio
async def test_review_success_path_with_fixes(
    config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(config, event_bus)
    transcript = (
        "Found issues, fixed them.\nVERDICT: APPROVE\nSUMMARY: Fixed and approved"
    )

    mock_execute = AsyncMock(return_value=transcript)
    mock_has_changes = AsyncMock(return_value=True)

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", mock_execute),
        patch.object(runner, "_has_changes", mock_has_changes),
        patch.object(runner, "_save_transcript"),
    ):
        result = await runner.review(pr_info, issue, tmp_path, "some diff")

    assert result.fixes_made is True
    assert result.verdict == ReviewVerdict.APPROVE


# ---------------------------------------------------------------------------
# review - failure path
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_review_failure_path_on_exception(
    config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(config, event_bus)

    mock_execute = AsyncMock(side_effect=RuntimeError("subprocess crashed"))

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", mock_execute),
    ):
        result = await runner.review(pr_info, issue, tmp_path, "some diff")

    assert result.verdict == ReviewVerdict.COMMENT
    assert "Review failed" in result.summary
    assert "subprocess crashed" in result.summary


# ---------------------------------------------------------------------------
# review - dry_run
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_review_dry_run_returns_auto_approved(
    dry_config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(dry_config, event_bus)
    mock_create = make_streaming_proc(returncode=0, stdout="")

    with patch("asyncio.create_subprocess_exec", mock_create):
        result = await runner.review(pr_info, issue, tmp_path, "some diff")

    mock_create.assert_not_called()
    assert result.verdict == ReviewVerdict.APPROVE
    assert result.summary == "Dry-run: auto-approved"
    assert result.pr_number == pr_info.number


# ---------------------------------------------------------------------------
# _save_transcript
# ---------------------------------------------------------------------------


def test_save_transcript_writes_to_correct_path(event_bus, tmp_path):
    cfg = ConfigFactory.create(repo_root=tmp_path)
    runner = ReviewRunner(config=cfg, event_bus=event_bus)
    transcript = "This is the review transcript."

    runner._save_transcript(42, transcript)

    expected_path = tmp_path / ".hydra" / "logs" / "review-pr-42.txt"
    assert expected_path.exists()
    assert expected_path.read_text() == transcript


def test_save_transcript_creates_log_directory(event_bus, tmp_path):
    cfg = ConfigFactory.create(repo_root=tmp_path)
    runner = ReviewRunner(config=cfg, event_bus=event_bus)
    log_dir = tmp_path / ".hydra" / "logs"
    assert not log_dir.exists()

    runner._save_transcript(7, "transcript content")

    assert log_dir.exists()
    assert log_dir.is_dir()


# ---------------------------------------------------------------------------
# REVIEW_UPDATE events
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_review_events_include_reviewer_role(
    config, event_bus, pr_info, issue, tmp_path
):
    """REVIEW_UPDATE events should carry role='reviewer'."""
    runner = _make_runner(config, event_bus)
    transcript = "All good.\nVERDICT: APPROVE\nSUMMARY: Looks great"

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_has_changes", AsyncMock(return_value=False)),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.review(pr_info, issue, tmp_path, "diff", worker_id=1)

    events = event_bus.get_history()
    review_events = [e for e in events if e.type == EventType.REVIEW_UPDATE]
    assert len(review_events) >= 2
    for event in review_events:
        assert event.data.get("role") == "reviewer"


@pytest.mark.asyncio
async def test_dry_run_review_events_include_reviewer_role(
    dry_config, event_bus, pr_info, issue, tmp_path
):
    """In dry-run mode, REVIEW_UPDATE events should still carry role='reviewer'."""
    runner = _make_runner(dry_config, event_bus)

    await runner.review(pr_info, issue, tmp_path, "diff")

    events = event_bus.get_history()
    review_events = [e for e in events if e.type == EventType.REVIEW_UPDATE]
    assert len(review_events) >= 1
    for event in review_events:
        assert event.data.get("role") == "reviewer"


@pytest.mark.asyncio
async def test_review_publishes_review_update_events(
    config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(config, event_bus)
    transcript = "All good.\nVERDICT: APPROVE\nSUMMARY: Looks great"

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_has_changes", AsyncMock(return_value=False)),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.review(pr_info, issue, tmp_path, "diff", worker_id=2)

    events = event_bus.get_history()
    review_events = [e for e in events if e.type == EventType.REVIEW_UPDATE]

    # Should have at least two: one for "reviewing" and one for "done"
    assert len(review_events) >= 2

    statuses = [e.data["status"] for e in review_events]
    assert "reviewing" in statuses
    assert "done" in statuses


@pytest.mark.asyncio
async def test_review_start_event_includes_worker_id(
    config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(config, event_bus)
    transcript = "VERDICT: APPROVE\nSUMMARY: ok"

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_has_changes", AsyncMock(return_value=False)),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.review(pr_info, issue, tmp_path, "diff", worker_id=3)

    events = event_bus.get_history()
    reviewing_event = next(
        e
        for e in events
        if e.type == EventType.REVIEW_UPDATE and e.data.get("status") == "reviewing"
    )
    assert reviewing_event.data["worker"] == 3
    assert reviewing_event.data["pr"] == pr_info.number
    assert reviewing_event.data["issue"] == issue.number


@pytest.mark.asyncio
async def test_review_done_event_includes_verdict_and_duration(
    config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(config, event_bus)
    transcript = "VERDICT: REQUEST_CHANGES\nSUMMARY: needs work"

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_has_changes", AsyncMock(return_value=False)),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.review(pr_info, issue, tmp_path, "diff")

    events = event_bus.get_history()
    done_event = next(
        e
        for e in events
        if e.type == EventType.REVIEW_UPDATE and e.data.get("status") == "done"
    )
    assert done_event.data["verdict"] == ReviewVerdict.REQUEST_CHANGES.value
    assert "duration" in done_event.data


@pytest.mark.asyncio
async def test_review_dry_run_still_publishes_review_update_event(
    dry_config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(dry_config, event_bus)

    await runner.review(pr_info, issue, tmp_path, "diff")

    events = event_bus.get_history()
    review_events = [e for e in events if e.type == EventType.REVIEW_UPDATE]
    # The "reviewing" event is published before the dry-run check
    assert any(e.data.get("status") == "reviewing" for e in review_events)


# ---------------------------------------------------------------------------
# _get_head_sha
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_get_head_sha_returns_sha(config, event_bus, tmp_path):
    runner = _make_runner(config, event_bus)
    mock_proc = AsyncMock()
    mock_proc.returncode = 0
    mock_proc.communicate = AsyncMock(return_value=(b"abc123def456\n", b""))
    mock_create = AsyncMock(return_value=mock_proc)

    with patch("asyncio.create_subprocess_exec", mock_create):
        result = await runner._get_head_sha(tmp_path)

    assert result == "abc123def456"


@pytest.mark.asyncio
async def test_get_head_sha_returns_none_on_failure(config, event_bus, tmp_path):
    runner = _make_runner(config, event_bus)
    mock_proc = AsyncMock()
    mock_proc.returncode = 128
    mock_proc.communicate = AsyncMock(return_value=(b"", b"fatal: not a git repo"))
    mock_create = AsyncMock(return_value=mock_proc)

    with patch("asyncio.create_subprocess_exec", mock_create):
        result = await runner._get_head_sha(tmp_path)

    assert result is None


@pytest.mark.asyncio
async def test_get_head_sha_returns_none_on_file_not_found(config, event_bus, tmp_path):
    runner = _make_runner(config, event_bus)
    mock_create = AsyncMock(side_effect=FileNotFoundError("git not found"))

    with patch("asyncio.create_subprocess_exec", mock_create):
        result = await runner._get_head_sha(tmp_path)

    assert result is None


# ---------------------------------------------------------------------------
# _has_changes
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_has_changes_true_when_head_moved(config, event_bus, tmp_path):
    runner = _make_runner(config, event_bus)

    with patch.object(runner, "_get_head_sha", AsyncMock(return_value="def456")):
        result = await runner._has_changes(tmp_path, before_sha="abc123")

    assert result is True


@pytest.mark.asyncio
async def test_has_changes_true_when_uncommitted_changes(config, event_bus, tmp_path):
    runner = _make_runner(config, event_bus)
    # Same SHA (no new commits), but dirty working tree
    mock_proc = AsyncMock()
    mock_proc.returncode = 0
    mock_proc.communicate = AsyncMock(return_value=(b" M foo.py\n", b""))
    mock_create = AsyncMock(return_value=mock_proc)

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch("asyncio.create_subprocess_exec", mock_create),
    ):
        result = await runner._has_changes(tmp_path, before_sha="abc123")

    assert result is True


@pytest.mark.asyncio
async def test_has_changes_false_when_clean(config, event_bus, tmp_path):
    runner = _make_runner(config, event_bus)
    # Same SHA and clean status
    mock_proc = AsyncMock()
    mock_proc.returncode = 0
    mock_proc.communicate = AsyncMock(return_value=(b"", b""))
    mock_create = AsyncMock(return_value=mock_proc)

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch("asyncio.create_subprocess_exec", mock_create),
    ):
        result = await runner._has_changes(tmp_path, before_sha="abc123")

    assert result is False


@pytest.mark.asyncio
async def test_has_changes_true_when_both_commits_and_dirty(
    config, event_bus, tmp_path
):
    runner = _make_runner(config, event_bus)
    # HEAD moved — should return True immediately without checking status

    with patch.object(runner, "_get_head_sha", AsyncMock(return_value="def456")):
        result = await runner._has_changes(tmp_path, before_sha="abc123")

    assert result is True


@pytest.mark.asyncio
async def test_has_changes_false_on_file_not_found(config, event_bus, tmp_path):
    runner = _make_runner(config, event_bus)

    with patch.object(
        runner, "_get_head_sha", AsyncMock(side_effect=FileNotFoundError)
    ):
        result = await runner._has_changes(tmp_path, before_sha="abc123")

    assert result is False


@pytest.mark.asyncio
async def test_has_changes_true_when_before_sha_none_and_dirty(
    config, event_bus, tmp_path
):
    runner = _make_runner(config, event_bus)
    # before_sha is None (e.g., empty repo) — falls through to status check
    mock_proc = AsyncMock()
    mock_proc.returncode = 0
    mock_proc.communicate = AsyncMock(return_value=(b"?? new_file.py\n", b""))
    mock_create = AsyncMock(return_value=mock_proc)

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch("asyncio.create_subprocess_exec", mock_create),
    ):
        result = await runner._has_changes(tmp_path, before_sha=None)

    assert result is True


@pytest.mark.asyncio
async def test_has_changes_false_when_before_sha_none_and_clean(
    config, event_bus, tmp_path
):
    runner = _make_runner(config, event_bus)
    # before_sha is None, clean status
    mock_proc = AsyncMock()
    mock_proc.returncode = 0
    mock_proc.communicate = AsyncMock(return_value=(b"", b""))
    mock_create = AsyncMock(return_value=mock_proc)

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch("asyncio.create_subprocess_exec", mock_create),
    ):
        result = await runner._has_changes(tmp_path, before_sha=None)

    assert result is False


# ---------------------------------------------------------------------------
# terminate
# ---------------------------------------------------------------------------


def test_terminate_kills_active_processes(config, event_bus):
    runner = _make_runner(config, event_bus)
    mock_proc = MagicMock()
    mock_proc.pid = 12345
    runner._active_procs.add(mock_proc)

    with patch("runner_utils.os.killpg") as mock_killpg:
        runner.terminate()

    mock_killpg.assert_called_once()


def test_terminate_handles_process_lookup_error(config, event_bus):
    runner = _make_runner(config, event_bus)
    mock_proc = MagicMock()
    mock_proc.pid = 12345
    runner._active_procs.add(mock_proc)

    with patch("runner_utils.os.killpg", side_effect=ProcessLookupError):
        runner.terminate()  # Should not raise


def test_terminate_with_no_active_processes(config, event_bus):
    runner = _make_runner(config, event_bus)
    runner.terminate()  # Should not raise


# ---------------------------------------------------------------------------
# _execute
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_execute_returns_transcript(config, event_bus, pr_info, tmp_path):
    runner = _make_runner(config, event_bus)
    expected_output = "VERDICT: APPROVE\nSUMMARY: looks good"
    mock_create = make_streaming_proc(returncode=0, stdout=expected_output)

    with patch("asyncio.create_subprocess_exec", mock_create):
        transcript = await runner._execute(
            ["claude", "-p"],
            "review prompt",
            tmp_path,
            pr_info.number,
        )

    assert transcript == expected_output


@pytest.mark.asyncio
async def test_execute_publishes_transcript_line_events(
    config, event_bus, pr_info, tmp_path
):
    runner = _make_runner(config, event_bus)
    output = "Line one\nLine two\nLine three"
    mock_create = make_streaming_proc(returncode=0, stdout=output)

    with patch("asyncio.create_subprocess_exec", mock_create):
        await runner._execute(
            ["claude", "-p"],
            "prompt",
            tmp_path,
            pr_info.number,
        )

    events = event_bus.get_history()
    transcript_events = [e for e in events if e.type == EventType.TRANSCRIPT_LINE]
    assert len(transcript_events) == 3
    lines = [e.data["line"] for e in transcript_events]
    assert "Line one" in lines
    assert "Line two" in lines
    assert "Line three" in lines
    # All events should carry the correct pr number and source
    for ev in transcript_events:
        assert ev.data["pr"] == pr_info.number
        assert ev.data["source"] == "reviewer"


@pytest.mark.asyncio
async def test_execute_uses_large_stream_limit(config, event_bus, pr_info, tmp_path):
    """_execute should set limit=1MB to handle large stream-json lines."""
    runner = _make_runner(config, event_bus)
    mock_create = make_streaming_proc(returncode=0, stdout="ok")

    with patch("asyncio.create_subprocess_exec", mock_create) as mock_exec:
        await runner._execute(["claude", "-p"], "prompt", tmp_path, pr_info.number)

    kwargs = mock_exec.call_args[1]
    assert kwargs["limit"] == 1024 * 1024


# ---------------------------------------------------------------------------
# _build_ci_fix_prompt
# ---------------------------------------------------------------------------


def test_build_ci_fix_prompt_includes_failure_summary(
    config, event_bus, pr_info, issue
):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_ci_fix_prompt(pr_info, issue, "Failed checks: ci, lint", 1)

    assert "Failed checks: ci, lint" in prompt


def test_build_ci_fix_prompt_includes_pr_and_issue_context(
    config, event_bus, pr_info, issue
):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_ci_fix_prompt(pr_info, issue, "CI failed", 2)

    assert f"#{pr_info.number}" in prompt
    assert f"#{issue.number}" in prompt
    assert issue.title in prompt
    assert "Attempt 2" in prompt


# ---------------------------------------------------------------------------
# fix_ci — success path
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_fix_ci_success_path(config, event_bus, pr_info, issue, tmp_path):
    runner = _make_runner(config, event_bus)
    transcript = "Fixed lint.\nVERDICT: APPROVE\nSUMMARY: Fixed CI failures"

    mock_execute = AsyncMock(return_value=transcript)
    mock_has_changes = AsyncMock(return_value=True)

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", mock_execute),
        patch.object(runner, "_has_changes", mock_has_changes),
        patch.object(runner, "_save_transcript"),
    ):
        result = await runner.fix_ci(
            pr_info, issue, tmp_path, "Failed: ci", attempt=1, worker_id=0
        )

    assert result.verdict == ReviewVerdict.APPROVE
    assert result.fixes_made is True
    assert result.summary == "Fixed CI failures"


# ---------------------------------------------------------------------------
# fix_ci — failure path
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_fix_ci_failure_path(config, event_bus, pr_info, issue, tmp_path):
    runner = _make_runner(config, event_bus)

    mock_execute = AsyncMock(side_effect=RuntimeError("agent crashed"))

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", mock_execute),
    ):
        result = await runner.fix_ci(pr_info, issue, tmp_path, "Failed: ci", attempt=1)

    assert result.verdict == ReviewVerdict.REQUEST_CHANGES
    assert "CI fix failed" in result.summary


# ---------------------------------------------------------------------------
# fix_ci — dry-run
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_fix_ci_dry_run_returns_auto_approved(
    dry_config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(dry_config, event_bus)

    result = await runner.fix_ci(pr_info, issue, tmp_path, "Failed: ci", attempt=1)

    assert result.verdict == ReviewVerdict.APPROVE
    assert "Dry-run" in result.summary


# ---------------------------------------------------------------------------
# fix_ci — CI_CHECK events
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_fix_ci_publishes_ci_check_events(
    config, event_bus, pr_info, issue, tmp_path
):
    runner = _make_runner(config, event_bus)
    transcript = "VERDICT: APPROVE\nSUMMARY: Fixed"

    with (
        patch.object(runner, "_get_head_sha", AsyncMock(return_value="abc123")),
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_has_changes", AsyncMock(return_value=True)),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.fix_ci(pr_info, issue, tmp_path, "Failed: ci", attempt=1)

    events = event_bus.get_history()
    ci_events = [e for e in events if e.type == EventType.CI_CHECK]
    assert len(ci_events) >= 2
    statuses = [e.data["status"] for e in ci_events]
    assert "fixing" in statuses
    assert "fix_done" in statuses
