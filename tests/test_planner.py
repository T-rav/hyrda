"""Tests for dx/hydra/planner.py."""

from __future__ import annotations

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from events import EventType
from models import PlannerStatus
from planner import PlannerRunner
from tests.helpers import ConfigFactory, make_streaming_proc

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _make_runner(config, event_bus):
    return PlannerRunner(config=config, event_bus=event_bus)


def _valid_plan(*, word_pad: int = 200) -> str:
    """Return a plan with all required sections that passes validation."""
    padding = " ".join(["word"] * max(0, word_pad - 80))
    return (
        "## Files to Modify\n\n"
        "- src/models.py — add new data model\n"
        "- src/config.py — add configuration field\n\n"
        "## New Files\n\n"
        "None\n\n"
        "## Implementation Steps\n\n"
        "1. Add the new model class to models.py\n"
        "2. Add configuration field to config.py\n"
        "3. Wire up the new model in the orchestrator\n"
        "4. Add validation logic\n\n"
        "## Testing Strategy\n\n"
        "- Add tests/test_models.py for the new model\n"
        "- Add tests/test_config.py for the new config field\n\n"
        "## Acceptance Criteria\n\n"
        "- New model can be created and serialized\n"
        "- Config field is validated correctly\n\n"
        "## Key Considerations\n\n"
        "- Backward compatibility with existing serialization\n"
        f"- Edge cases around empty values\n\n{padding}"
    )


def _valid_transcript(*, word_pad: int = 200) -> str:
    """Return a transcript containing a valid plan."""
    return (
        "Analysis complete.\n"
        f"PLAN_START\n{_valid_plan(word_pad=word_pad)}\nPLAN_END\n"
        "SUMMARY: Implementation plan for the feature"
    )


# ---------------------------------------------------------------------------
# _build_command
# ---------------------------------------------------------------------------


def test_build_command_uses_planner_model_and_budget(config):
    runner = _make_runner(config, None)
    cmd = runner._build_command()

    assert "claude" in cmd
    assert "-p" in cmd
    assert "--model" in cmd
    model_idx = cmd.index("--model")
    assert cmd[model_idx + 1] == config.planner_model

    assert "--max-budget-usd" in cmd
    budget_idx = cmd.index("--max-budget-usd")
    assert cmd[budget_idx + 1] == str(config.planner_budget_usd)


def test_build_command_omits_budget_when_zero(tmp_path):
    from tests.conftest import ConfigFactory

    cfg = ConfigFactory.create(
        planner_budget_usd=0,
        repo_root=tmp_path / "repo",
        worktree_base=tmp_path / "wt",
        state_file=tmp_path / "s.json",
    )
    runner = _make_runner(cfg, None)
    cmd = runner._build_command()
    assert "--max-budget-usd" not in cmd


def test_build_command_includes_output_format(config):
    runner = _make_runner(config, None)
    cmd = runner._build_command()

    assert "--output-format" in cmd
    fmt_idx = cmd.index("--output-format")
    assert cmd[fmt_idx + 1] == "stream-json"


def test_build_command_includes_verbose(config):
    runner = _make_runner(config, None)
    cmd = runner._build_command()

    assert "--verbose" in cmd


def test_build_command_disallows_write_tools(config):
    runner = _make_runner(config, None)
    cmd = runner._build_command()

    assert "--disallowedTools" in cmd
    idx = cmd.index("--disallowedTools")
    blocked = cmd[idx + 1]
    assert "Write" in blocked
    assert "Edit" in blocked
    assert "NotebookEdit" in blocked


# ---------------------------------------------------------------------------
# _build_prompt
# ---------------------------------------------------------------------------


def test_build_prompt_includes_issue_number(config, event_bus, issue):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert f"#{issue.number}" in prompt


def test_build_prompt_includes_issue_context(config, event_bus, issue):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert issue.title in prompt
    assert issue.body in prompt


def test_build_prompt_includes_read_only_instructions(config, event_bus, issue):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "READ-ONLY" in prompt
    assert "Do NOT create, modify, or delete any files" in prompt


def test_build_prompt_includes_plan_markers(config, event_bus, issue):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "PLAN_START" in prompt
    assert "PLAN_END" in prompt
    assert "SUMMARY:" in prompt


def test_build_prompt_includes_comments_when_present(config, event_bus):
    from models import GitHubIssue

    issue = GitHubIssue(
        number=42,
        title="Fix the frobnicator",
        body="It is broken.",
        comments=["First comment", "Second comment"],
    )
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "First comment" in prompt
    assert "Second comment" in prompt
    assert "Discussion" in prompt


def test_build_prompt_omits_comments_section_when_empty(config, event_bus, issue):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "Discussion" not in prompt


def test_build_prompt_truncates_long_body(config, event_bus):
    from models import GitHubIssue

    issue = GitHubIssue(
        number=1, title="Big issue", body="X" * 20_000, labels=[], comments=[], url=""
    )
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "…(truncated)" in prompt
    assert len(prompt) < 10_000  # well under original 20k body


def test_build_prompt_truncates_long_comments(config, event_bus):
    from models import GitHubIssue

    issue = GitHubIssue(
        number=1,
        title="Big comments",
        body="Normal body with enough content",
        labels=[],
        comments=["C" * 5000, "Short"],
        url="",
    )
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    # First comment should be truncated, second should be intact
    assert "…" in prompt
    assert "Short" in prompt


def test_build_prompt_truncates_long_lines(config, event_bus):
    """Lines exceeding _MAX_LINE_CHARS are hard-truncated to prevent
    Claude CLI text-splitter failures."""
    from models import GitHubIssue

    long_line = "A" * 2000
    body = f"Short line\n{long_line}\nAnother short line"
    issue = GitHubIssue(
        number=1, title="Long lines", body=body, labels=[], comments=[], url=""
    )
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    # No line in the prompt should exceed _MAX_LINE_CHARS + ellipsis
    for line in prompt.splitlines():
        assert len(line) <= runner._MAX_LINE_CHARS + 10  # small margin for marker text


def test_truncate_text_respects_line_boundaries():
    """_truncate_text cuts at line boundaries, not mid-line."""
    text = "line1\nline2\nline3\nline4\nline5"
    result = PlannerRunner._truncate_text(text, char_limit=18, line_limit=500)
    # Should include line1 (5) + \n + line2 (5) + \n + line3 (5) = 17 chars
    assert "line1" in result
    assert "line2" in result
    assert "line3" in result
    assert "line4" not in result or "…(truncated)" in result


def test_truncate_text_no_truncation_when_under_limit():
    """_truncate_text returns text unchanged when under limits."""
    text = "short text"
    result = PlannerRunner._truncate_text(text, char_limit=500, line_limit=500)
    assert result == text
    assert "…(truncated)" not in result


# ---------------------------------------------------------------------------
# _build_prompt - image handling
# ---------------------------------------------------------------------------


def test_build_prompt_notes_images_in_body(config, event_bus):
    """When the issue body contains markdown images, the prompt should note them."""
    from models import GitHubIssue

    issue = GitHubIssue(
        number=99,
        title="Fix layout bug",
        body="The layout is broken.\n\n![screenshot](https://example.com/img.png)\n\nSee above.",
        labels=[],
        comments=[],
        url="",
    )
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "image" in prompt.lower() or "screenshot" in prompt.lower()
    assert "visual" in prompt.lower() or "attached" in prompt.lower()


def test_build_prompt_notes_html_images_in_body(config, event_bus):
    """When the issue body contains HTML img tags, the prompt should note them."""
    from models import GitHubIssue

    issue = GitHubIssue(
        number=99,
        title="Fix layout bug",
        body='See screenshot:\n\n<img src="https://example.com/img.png" />\n\nPlease fix.',
        labels=[],
        comments=[],
        url="",
    )
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "image" in prompt.lower() or "screenshot" in prompt.lower()


def test_build_prompt_no_image_note_when_no_images(config, event_bus, issue):
    """When the issue body has no images, no image note should be added."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "image" not in prompt.lower() or "image" in issue.body.lower()
    # The specific note about attached images should not appear
    assert "visual context" not in prompt.lower()


def test_build_prompt_handles_multiple_images(config, event_bus):
    """Multiple images in the body should still produce a single note."""
    from models import GitHubIssue

    issue = GitHubIssue(
        number=99,
        title="Fix layout bug",
        body="![img1](https://example.com/1.png)\n![img2](https://example.com/2.png)",
        labels=[],
        comments=[],
        url="",
    )
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    # Should mention images
    assert "image" in prompt.lower()


# ---------------------------------------------------------------------------
# _build_prompt - UI exploration guidance
# ---------------------------------------------------------------------------


def test_build_prompt_includes_ui_exploration_guidance(config, event_bus, issue):
    """Planner prompt should include UI exploration patterns."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "ui/src/components/" in prompt
    assert "constants.js" in prompt
    assert "theme.js" in prompt
    assert "types.js" in prompt


# ---------------------------------------------------------------------------
# _extract_plan
# ---------------------------------------------------------------------------


def test_extract_plan_with_markers(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "Some preamble\nPLAN_START\nStep 1: Do this\nStep 2: Do that\nPLAN_END\nSome epilogue"
    plan = runner._extract_plan(transcript)

    assert plan == "Step 1: Do this\nStep 2: Do that"


def test_extract_plan_without_markers_returns_empty(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "Here is the full plan without markers.\nLine 2."
    plan = runner._extract_plan(transcript)

    assert plan == ""


def test_extract_plan_budget_exceeded_returns_empty(config, event_bus):
    """Budget-exceeded error output must not be treated as a plan."""
    runner = _make_runner(config, event_bus)
    transcript = "Error: Exceeded USD budget (3)"
    plan = runner._extract_plan(transcript)

    assert plan == ""


def test_extract_plan_multiline(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = (
        "Analysis:\nPLAN_START\n"
        "## Files to modify\n\n"
        "- models.py: Add new class\n"
        "- tests/test_models.py: Add tests\n\n"
        "## Steps\n\n"
        "1. Create the model\n"
        "2. Write tests\n"
        "PLAN_END\n"
        "SUMMARY: Add new model"
    )
    plan = runner._extract_plan(transcript)

    assert "## Files to modify" in plan
    assert "## Steps" in plan
    assert "SUMMARY" not in plan


def test_extract_plan_empty_transcript(config, event_bus):
    runner = _make_runner(config, event_bus)
    plan = runner._extract_plan("")

    assert plan == ""


# ---------------------------------------------------------------------------
# _extract_summary
# ---------------------------------------------------------------------------


def test_extract_summary_with_summary_line(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "Plan done.\nPLAN_END\nSUMMARY: implement the widget feature"
    summary = runner._extract_summary(transcript)

    assert summary == "implement the widget feature"


def test_extract_summary_case_insensitive(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "summary: all changes identified"
    summary = runner._extract_summary(transcript)

    assert summary == "all changes identified"


def test_extract_summary_fallback_to_last_line(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "First line.\nSecond line.\nThis is the last line"
    summary = runner._extract_summary(transcript)

    assert summary == "This is the last line"


def test_extract_summary_empty_transcript(config, event_bus):
    runner = _make_runner(config, event_bus)
    summary = runner._extract_summary("")

    assert summary == "No summary provided"


# ---------------------------------------------------------------------------
# _extract_already_satisfied
# ---------------------------------------------------------------------------


def test_extract_already_satisfied_with_markers():
    transcript = (
        "Analysis complete.\n"
        "ALREADY_SATISFIED_START\n"
        "The feature described in this issue is already implemented "
        "in src/models.py lines 10-25.\n"
        "ALREADY_SATISFIED_END\n"
        "Done."
    )
    result = PlannerRunner._extract_already_satisfied(transcript)
    assert "already implemented" in result
    assert "src/models.py" in result


def test_extract_already_satisfied_without_markers():
    transcript = "Analysis complete.\nPLAN_START\nStep 1\nPLAN_END\nSUMMARY: Done"
    result = PlannerRunner._extract_already_satisfied(transcript)
    assert result == ""


def test_extract_already_satisfied_empty_transcript():
    result = PlannerRunner._extract_already_satisfied("")
    assert result == ""


def test_extract_already_satisfied_strips_whitespace():
    transcript = (
        "ALREADY_SATISFIED_START\n"
        "  The code already handles this.  \n"
        "ALREADY_SATISFIED_END"
    )
    result = PlannerRunner._extract_already_satisfied(transcript)
    assert result == "The code already handles this."


def test_extract_already_satisfied_multiline():
    transcript = (
        "ALREADY_SATISFIED_START\n"
        "Line 1 of explanation.\n"
        "Line 2 of explanation.\n"
        "Line 3 of explanation.\n"
        "ALREADY_SATISFIED_END"
    )
    result = PlannerRunner._extract_already_satisfied(transcript)
    assert "Line 1" in result
    assert "Line 2" in result
    assert "Line 3" in result


# ---------------------------------------------------------------------------
# _build_prompt - already satisfied markers
# ---------------------------------------------------------------------------


def test_build_prompt_includes_already_satisfied_markers(config, event_bus, issue):
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "ALREADY_SATISFIED_START" in prompt
    assert "ALREADY_SATISFIED_END" in prompt


def test_build_prompt_lite_includes_already_satisfied_markers(config, event_bus):
    """Lite prompt (for bug/typo labels) should also include markers."""
    from models import GitHubIssue

    issue = GitHubIssue(
        number=42,
        title="Fix typo",
        body="There's a typo in the docs.",
        labels=["bug"],
        comments=[],
    )
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "ALREADY_SATISFIED_START" in prompt
    assert "ALREADY_SATISFIED_END" in prompt


# ---------------------------------------------------------------------------
# _significant_words
# ---------------------------------------------------------------------------


def test_significant_words_extracts_long_words(config, event_bus):
    runner = _make_runner(config, event_bus)
    words = runner._significant_words("Fix the broken authentication handler")
    assert "broken" in words
    assert "authentication" in words
    assert "handler" in words
    # "the" is too short (< 4 chars)
    assert "the" not in words
    # "Fix" is too short
    assert "fix" not in words


def test_significant_words_filters_stop_words(config, event_bus):
    runner = _make_runner(config, event_bus)
    words = runner._significant_words("This should have been done with more care")
    # All are stop words or short
    assert "this" not in words
    assert "should" not in words
    assert "have" not in words
    assert "been" not in words
    assert "with" not in words
    assert "more" not in words
    assert "care" in words
    assert "done" in words


def test_significant_words_empty_string(config, event_bus):
    runner = _make_runner(config, event_bus)
    words = runner._significant_words("")
    assert words == set()


# ---------------------------------------------------------------------------
# _validate_plan — schema validation
# ---------------------------------------------------------------------------


def test_validate_plan_all_sections_present(config, event_bus):
    """A valid plan with all 6 sections and sufficient words passes."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix authentication handler")
    errors = runner._validate_plan(issue, _valid_plan())
    assert errors == []


def test_validate_plan_missing_section_returns_errors(config, event_bus):
    """Plan missing '## Testing Strategy' returns that specific error."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix authentication handler")
    plan = _valid_plan().replace("## Testing Strategy", "## Tests")
    errors = runner._validate_plan(issue, plan)
    assert any("Testing Strategy" in e for e in errors)


def test_validate_plan_missing_multiple_sections(config, event_bus):
    """Plan missing 3 sections returns 3 corresponding errors."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix auth")
    plan = _valid_plan()
    plan = plan.replace("## Testing Strategy", "## Tests")
    plan = plan.replace("## Acceptance Criteria", "## Done")
    plan = plan.replace("## Key Considerations", "## Notes")
    errors = runner._validate_plan(issue, plan)
    missing = [e for e in errors if "Missing required section" in e]
    assert len(missing) == 3


def test_validate_plan_files_to_modify_requires_file_path(config, event_bus):
    """Files to Modify section present but with no file paths fails."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix it")
    plan = _valid_plan().replace(
        "- src/models.py — add new data model\n"
        "- src/config.py — add configuration field",
        "- Some vague description without paths",
    )
    errors = runner._validate_plan(issue, plan)
    assert any("file path" in e for e in errors)


def test_validate_plan_testing_strategy_requires_test_reference(config, event_bus):
    """Testing Strategy section present but with no test file references fails."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix it")
    plan = _valid_plan().replace(
        "- Add tests/test_models.py for the new model\n"
        "- Add tests/test_config.py for the new config field",
        "- Write some unit checks\n- Verify behavior manually",
    )
    errors = runner._validate_plan(issue, plan)
    assert any("test file" in e for e in errors)


def test_validate_plan_implementation_steps_requires_three(config, event_bus):
    """Less than 3 numbered steps fails."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix it")
    plan = _valid_plan().replace(
        "1. Add the new model class to models.py\n"
        "2. Add configuration field to config.py\n"
        "3. Wire up the new model in the orchestrator\n"
        "4. Add validation logic",
        "1. Do the thing\n2. Done",
    )
    errors = runner._validate_plan(issue, plan)
    assert any("3 numbered steps" in e for e in errors)


def test_validate_plan_minimum_word_count(config, event_bus):
    """Plan below min_plan_words fails."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix it")
    # Create a short plan that has all sections but few words
    short_plan = (
        "## Files to Modify\n\n- src/app.py — fix\n\n"
        "## New Files\n\nNone\n\n"
        "## Implementation Steps\n\n1. A\n2. B\n3. C\n\n"
        "## Testing Strategy\n\n- tests/test_app.py\n\n"
        "## Acceptance Criteria\n\n- Done\n\n"
        "## Key Considerations\n\n- None\n"
    )
    errors = runner._validate_plan(issue, short_plan)
    assert any("words" in e for e in errors)


def test_validate_plan_word_count_configurable(event_bus, tmp_path):
    """Custom min_plan_words is respected."""
    from models import GitHubIssue

    cfg = ConfigFactory.create(min_plan_words=50, repo_root=tmp_path)
    runner = _make_runner(cfg, event_bus)
    issue = GitHubIssue(number=1, title="Fix it")
    # This plan has all sections but only ~50 words
    plan = (
        "## Files to Modify\n\n- src/app.py — fix bug\n\n"
        "## New Files\n\nNone\n\n"
        "## Implementation Steps\n\n1. Fix the bug in app.py\n"
        "2. Update the tests for the fix\n"
        "3. Run the test suite to verify\n\n"
        "## Testing Strategy\n\n- tests/test_app.py covers the fix\n\n"
        "## Acceptance Criteria\n\n- Bug is fixed\n\n"
        "## Key Considerations\n\n- Backward compatibility\n"
    )
    errors = runner._validate_plan(issue, plan)
    assert not any("words" in e for e in errors)


def test_validate_plan_logs_warning_on_word_mismatch(config, event_bus):
    """Word-overlap check logs a warning but doesn't produce errors."""
    from unittest.mock import patch as mock_patch

    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix authentication handler")
    # Valid plan but title words don't overlap with plan
    plan = _valid_plan().replace("authentication", "database")

    with mock_patch("planner.logger") as mock_logger:
        runner._validate_plan(issue, plan)

    mock_logger.warning.assert_called_once()


# ---------------------------------------------------------------------------
# _extract_new_issues
# ---------------------------------------------------------------------------


def test_extract_new_issues_with_markers(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = (
        "PLAN_START\nStep 1\nPLAN_END\n"
        "NEW_ISSUES_START\n"
        "- title: Fix the widget\n"
        "  body: The widget is broken\n"
        "  labels: bug, high-priority\n"
        "- title: Refactor auth module\n"
        "  body: Needs cleanup\n"
        "  labels: tech-debt\n"
        "NEW_ISSUES_END\n"
        "SUMMARY: done"
    )
    issues = runner._extract_new_issues(transcript)
    assert len(issues) == 2
    assert issues[0].title == "Fix the widget"
    assert issues[0].body == "The widget is broken"
    assert "bug" in issues[0].labels
    assert "high-priority" in issues[0].labels
    assert issues[1].title == "Refactor auth module"
    assert "tech-debt" in issues[1].labels


def test_extract_new_issues_without_markers(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = "PLAN_START\nStep 1\nPLAN_END\nSUMMARY: done"
    issues = runner._extract_new_issues(transcript)
    assert issues == []


def test_extract_new_issues_single_issue(config, event_bus):
    runner = _make_runner(config, event_bus)
    transcript = (
        "NEW_ISSUES_START\n"
        "- title: Add logging\n"
        "  body: We need more logging\n"
        "  labels: enhancement\n"
        "NEW_ISSUES_END"
    )
    issues = runner._extract_new_issues(transcript)
    assert len(issues) == 1
    assert issues[0].title == "Add logging"


def test_extract_new_issues_multiline_body(config, event_bus):
    """Multi-line body continuation lines are concatenated."""
    runner = _make_runner(config, event_bus)
    transcript = (
        "NEW_ISSUES_START\n"
        "- title: Fix the widget\n"
        "  body: The widget is broken in production. Users are seeing\n"
        "    errors when they click the submit button because the form\n"
        "    validation skips required fields.\n"
        "  labels: bug\n"
        "NEW_ISSUES_END"
    )
    issues = runner._extract_new_issues(transcript)
    assert len(issues) == 1
    assert issues[0].title == "Fix the widget"
    assert "widget is broken" in issues[0].body
    assert "form" in issues[0].body
    assert "validation" in issues[0].body
    assert len(issues[0].body) > 50


# ---------------------------------------------------------------------------
# plan - success path
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_plan_success_path(config, event_bus, issue, tmp_path):
    runner = _make_runner(config, event_bus)
    transcript = _valid_transcript()

    mock_execute = AsyncMock(return_value=transcript)

    with (
        patch.object(runner, "_execute", mock_execute),
        patch.object(runner, "_save_transcript"),
    ):
        result = await runner.plan(issue, worker_id=0)

    assert result.issue_number == issue.number
    assert result.success is True
    assert "## Files to Modify" in result.plan
    assert result.summary == "Implementation plan for the feature"
    assert result.validation_errors == []


# ---------------------------------------------------------------------------
# plan - failure path
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_plan_failure_on_exception(config, event_bus, issue, tmp_path):
    runner = _make_runner(config, event_bus)

    mock_execute = AsyncMock(side_effect=RuntimeError("subprocess crashed"))

    with patch.object(runner, "_execute", mock_execute):
        result = await runner.plan(issue, worker_id=0)

    assert result.success is False
    assert result.error == "subprocess crashed"


# ---------------------------------------------------------------------------
# plan - dry_run
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_plan_dry_run(dry_config, event_bus, issue, tmp_path):
    runner = _make_runner(dry_config, event_bus)
    mock_create = make_streaming_proc(returncode=0, stdout="")

    with patch("asyncio.create_subprocess_exec", mock_create):
        result = await runner.plan(issue, worker_id=0)

    mock_create.assert_not_called()
    assert result.success is True
    assert result.summary == "Dry-run: plan skipped"


# ---------------------------------------------------------------------------
# plan - already_satisfied path
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_plan_already_satisfied_sets_flag_and_skips_validation(
    config, event_bus, issue
):
    """When transcript contains ALREADY_SATISFIED markers, plan() should
    set already_satisfied=True, success=True, and skip plan extraction."""
    runner = _make_runner(config, event_bus)
    transcript = (
        "Analysis complete.\n"
        "ALREADY_SATISFIED_START\n"
        "The feature is already implemented in src/models.py lines 10-25.\n"
        "ALREADY_SATISFIED_END\n"
    )

    mock_execute = AsyncMock(return_value=transcript)

    with (
        patch.object(runner, "_execute", mock_execute),
        patch.object(runner, "_save_transcript"),
    ):
        result = await runner.plan(issue, worker_id=0)

    assert result.already_satisfied is True
    assert result.success is True
    assert result.plan == ""  # no plan extracted
    assert "already implemented" in result.summary


@pytest.mark.asyncio
async def test_plan_already_satisfied_does_not_extract_plan(config, event_bus, issue):
    """When already_satisfied markers are present, _extract_plan should NOT be called."""
    runner = _make_runner(config, event_bus)
    transcript = "ALREADY_SATISFIED_START\nAlready done.\nALREADY_SATISFIED_END\n"

    mock_execute = AsyncMock(return_value=transcript)

    with (
        patch.object(runner, "_execute", mock_execute),
        patch.object(runner, "_save_transcript"),
        patch.object(runner, "_extract_plan") as mock_extract,
    ):
        result = await runner.plan(issue, worker_id=0)

    mock_extract.assert_not_called()
    assert result.already_satisfied is True


# ---------------------------------------------------------------------------
# _save_transcript
# ---------------------------------------------------------------------------


def test_save_transcript_writes_to_correct_path(event_bus, tmp_path):
    cfg = ConfigFactory.create(repo_root=tmp_path)
    runner = PlannerRunner(config=cfg, event_bus=event_bus)
    transcript = "This is the planning transcript."

    runner._save_transcript(42, transcript)

    expected_path = tmp_path / ".hydra" / "logs" / "plan-issue-42.txt"
    assert expected_path.exists()
    assert expected_path.read_text() == transcript


def test_save_transcript_creates_log_directory(event_bus, tmp_path):
    cfg = ConfigFactory.create(repo_root=tmp_path)
    runner = PlannerRunner(config=cfg, event_bus=event_bus)
    log_dir = tmp_path / ".hydra" / "logs"
    assert not log_dir.exists()

    runner._save_transcript(7, "transcript content")

    assert log_dir.exists()
    assert log_dir.is_dir()


def test_save_transcript_handles_oserror(event_bus, tmp_path, caplog):
    cfg = ConfigFactory.create(repo_root=tmp_path)
    runner = PlannerRunner(config=cfg, event_bus=event_bus)

    with patch.object(Path, "write_text", side_effect=OSError("disk full")):
        runner._save_transcript(42, "transcript")  # should not raise

    assert "Could not save transcript" in caplog.text


# ---------------------------------------------------------------------------
# _save_plan
# ---------------------------------------------------------------------------


def test_save_plan_writes_to_correct_path(event_bus, tmp_path):
    cfg = ConfigFactory.create(repo_root=tmp_path)
    runner = PlannerRunner(config=cfg, event_bus=event_bus)

    runner._save_plan(42, "Step 1: Do X\nStep 2: Do Y", "Two-step plan")

    expected_path = tmp_path / ".hydra" / "plans" / "issue-42.md"
    assert expected_path.exists()
    content = expected_path.read_text()
    assert "Step 1: Do X" in content
    assert "Two-step plan" in content


def test_save_plan_creates_plans_directory(event_bus, tmp_path):
    cfg = ConfigFactory.create(repo_root=tmp_path)
    runner = PlannerRunner(config=cfg, event_bus=event_bus)
    plan_dir = tmp_path / ".hydra" / "plans"
    assert not plan_dir.exists()

    runner._save_plan(7, "Some plan", "Summary")

    assert plan_dir.exists()
    assert plan_dir.is_dir()


def test_save_plan_handles_oserror(event_bus, tmp_path, caplog):
    cfg = ConfigFactory.create(repo_root=tmp_path)
    runner = PlannerRunner(config=cfg, event_bus=event_bus)

    with patch.object(Path, "write_text", side_effect=OSError("disk full")):
        runner._save_plan(42, "Some plan", "Summary")  # should not raise

    assert "Could not save plan" in caplog.text


# ---------------------------------------------------------------------------
# plan() — _save_transcript / _save_plan OSError defense-in-depth
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_plan_returns_result_when_save_transcript_raises_os_error(
    config, event_bus, issue, caplog
):
    """plan() should return a valid PlanResult even if _save_transcript raises OSError."""
    runner = _make_runner(config, event_bus)
    transcript = _valid_transcript()

    with (
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_save_transcript", side_effect=OSError("disk full")),
    ):
        result = await runner.plan(issue, worker_id=0)

    assert result.success is True
    assert result.issue_number == issue.number
    assert "## Files to Modify" in result.plan
    assert "Failed to save transcript" in caplog.text


@pytest.mark.asyncio
async def test_plan_returns_result_when_save_plan_raises_os_error(
    config, event_bus, issue, caplog
):
    """plan() should return a successful PlanResult even if _save_plan raises OSError."""
    runner = _make_runner(config, event_bus)
    transcript = _valid_transcript()

    with (
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_save_transcript"),
        patch.object(runner, "_save_plan", side_effect=OSError("disk full")),
    ):
        result = await runner.plan(issue, worker_id=0)

    assert result.success is True
    assert result.issue_number == issue.number
    assert "## Files to Modify" in result.plan
    assert "Failed to save plan" in caplog.text


# ---------------------------------------------------------------------------
# PLANNER_UPDATE events
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_planner_events_include_planner_role(config, event_bus, issue, tmp_path):
    """PLANNER_UPDATE events should carry role='planner'."""
    runner = _make_runner(config, event_bus)
    transcript = _valid_transcript()

    with (
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.plan(issue, worker_id=1)

    events = event_bus.get_history()
    planner_events = [e for e in events if e.type == EventType.PLANNER_UPDATE]
    assert len(planner_events) >= 2
    for event in planner_events:
        assert event.data.get("role") == "planner"


@pytest.mark.asyncio
async def test_plan_emits_planning_and_done_events(config, event_bus, issue, tmp_path):
    """Status should go through planning → done on success."""
    runner = _make_runner(config, event_bus)
    transcript = _valid_transcript()

    with (
        patch.object(runner, "_execute", AsyncMock(return_value=transcript)),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.plan(issue, worker_id=0)

    events = event_bus.get_history()
    planner_events = [e for e in events if e.type == EventType.PLANNER_UPDATE]

    statuses = [e.data["status"] for e in planner_events]
    assert PlannerStatus.PLANNING.value in statuses
    assert PlannerStatus.DONE.value in statuses


@pytest.mark.asyncio
async def test_plan_emits_planning_and_failed_events_on_error(
    config, event_bus, issue, tmp_path
):
    """Status should go through planning → failed on exception."""
    runner = _make_runner(config, event_bus)

    with patch.object(runner, "_execute", AsyncMock(side_effect=RuntimeError("boom"))):
        await runner.plan(issue, worker_id=0)

    events = event_bus.get_history()
    planner_events = [e for e in events if e.type == EventType.PLANNER_UPDATE]

    statuses = [e.data["status"] for e in planner_events]
    assert PlannerStatus.PLANNING.value in statuses
    assert PlannerStatus.FAILED.value in statuses


# ---------------------------------------------------------------------------
# terminate
# ---------------------------------------------------------------------------


def test_terminate_kills_active_processes(config, event_bus):
    runner = _make_runner(config, event_bus)
    mock_proc = MagicMock()
    mock_proc.pid = 12345
    runner._active_procs.add(mock_proc)

    with patch("runner_utils.os.killpg") as mock_killpg:
        runner.terminate()

    mock_killpg.assert_called_once()


def test_terminate_handles_process_lookup_error(config, event_bus):
    runner = _make_runner(config, event_bus)
    mock_proc = MagicMock()
    mock_proc.pid = 12345
    runner._active_procs.add(mock_proc)

    with patch("runner_utils.os.killpg", side_effect=ProcessLookupError):
        runner.terminate()  # Should not raise


def test_terminate_with_no_active_processes(config, event_bus):
    runner = _make_runner(config, event_bus)
    runner.terminate()  # Should not raise


# ---------------------------------------------------------------------------
# _execute - transcript lines
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_execute_publishes_transcript_lines(config, event_bus, issue, tmp_path):
    runner = _make_runner(config, event_bus)
    output = "Line one\nLine two\nLine three"
    mock_create = make_streaming_proc(returncode=0, stdout=output)

    with patch("asyncio.create_subprocess_exec", mock_create):
        transcript = await runner._execute(
            ["claude", "-p"],
            "prompt",
            tmp_path,
            issue.number,
        )

    assert transcript == output

    events = event_bus.get_history()
    transcript_events = [e for e in events if e.type == EventType.TRANSCRIPT_LINE]
    assert len(transcript_events) == 3
    for ev in transcript_events:
        assert ev.data["source"] == "planner"
        assert ev.data["issue"] == issue.number


@pytest.mark.asyncio
async def test_execute_uses_large_stream_limit(config, event_bus, issue, tmp_path):
    """_execute should set limit=1MB to handle large stream-json lines."""
    runner = _make_runner(config, event_bus)
    mock_create = make_streaming_proc(returncode=0, stdout="ok")

    with patch("asyncio.create_subprocess_exec", mock_create) as mock_exec:
        await runner._execute(["claude", "-p"], "prompt", tmp_path, issue.number)

    kwargs = mock_exec.call_args[1]
    assert kwargs["limit"] == 1024 * 1024


# ---------------------------------------------------------------------------
# Phase -1 gates
# ---------------------------------------------------------------------------


def test_phase_minus_one_simplicity_gate_warns_on_many_new_files(config, event_bus):
    """Creating > max_new_files_warning new files produces a warning."""
    runner = _make_runner(config, event_bus)
    plan = _valid_plan().replace(
        "## New Files\n\nNone",
        "## New Files\n\n"
        "- src/new1.py\n- src/new2.py\n- src/new3.py\n"
        "- src/new4.py\n- src/new5.py\n- src/new6.py",
    )
    blocking, warnings = runner._run_phase_minus_one_gates(plan)
    assert not blocking
    assert any("Simplicity gate" in w for w in warnings)


def test_phase_minus_one_test_first_gate_rejects_later(config, event_bus):
    """'tests will be added later' in Testing Strategy is rejected."""
    runner = _make_runner(config, event_bus)
    plan = _valid_plan().replace(
        "- Add tests/test_models.py for the new model\n"
        "- Add tests/test_config.py for the new config field",
        "Tests will be added later after implementation is stable.",
    )
    blocking, _ = runner._run_phase_minus_one_gates(plan)
    assert any("Test-first gate" in e for e in blocking)


def test_phase_minus_one_test_first_gate_rejects_tbd(config, event_bus):
    """'TBD' in Testing Strategy is rejected."""
    runner = _make_runner(config, event_bus)
    plan = _valid_plan().replace(
        "- Add tests/test_models.py for the new model\n"
        "- Add tests/test_config.py for the new config field",
        "TBD",
    )
    blocking, _ = runner._run_phase_minus_one_gates(plan)
    assert any("Test-first gate" in e for e in blocking)


def test_phase_minus_one_test_first_gate_accepts_valid(config, event_bus):
    """A proper testing strategy passes the test-first gate."""
    runner = _make_runner(config, event_bus)
    blocking, _ = runner._run_phase_minus_one_gates(_valid_plan())
    assert not any("Test-first gate" in e for e in blocking)


def test_phase_minus_one_constitution_gate_skipped_when_no_file(config, event_bus):
    """No constitution.md = gate passes with no errors."""
    runner = _make_runner(config, event_bus)
    blocking, warnings = runner._run_phase_minus_one_gates(_valid_plan())
    assert not any("Constitution gate" in e for e in blocking)
    assert not any("Constitution gate" in w for w in warnings)


def test_phase_minus_one_constitution_gate_reads_file(event_bus, tmp_path):
    """When constitution.md exists, its principles are checked against the plan."""
    cfg = ConfigFactory.create(repo_root=tmp_path)
    (tmp_path / "constitution.md").write_text(
        "# Principles\n\n- never delete user data\n- always validate input\n"
    )
    runner = _make_runner(cfg, event_bus)
    plan = _valid_plan() + "\nWe will never delete user data from the database.\n"
    blocking, _ = runner._run_phase_minus_one_gates(plan)
    assert any("Constitution gate" in e for e in blocking)


# ---------------------------------------------------------------------------
# Retry flow
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_plan_retries_on_validation_failure(config, event_bus, issue):
    """First attempt fails validation, second succeeds."""
    runner = _make_runner(config, event_bus)

    bad_transcript = "PLAN_START\nJust a one-liner.\nPLAN_END\nSUMMARY: bad"
    good_transcript = _valid_transcript()

    call_count = {"n": 0}

    async def mock_execute(cmd, prompt, cwd, issue_number):
        call_count["n"] += 1
        if call_count["n"] == 1:
            return bad_transcript
        return good_transcript

    with (
        patch.object(runner, "_execute", side_effect=mock_execute),
        patch.object(runner, "_save_transcript"),
    ):
        result = await runner.plan(issue, worker_id=0)

    assert result.success is True
    assert call_count["n"] == 2
    assert result.retry_attempted is False


@pytest.mark.asyncio
async def test_plan_retry_prompt_includes_feedback(config, event_bus, issue):
    """Retry prompt contains specific validation errors."""
    runner = _make_runner(config, event_bus)

    bad_transcript = "PLAN_START\nJust a one-liner.\nPLAN_END\nSUMMARY: bad"

    prompts_used: list[str] = []

    async def mock_execute(cmd, prompt, cwd, issue_number):
        prompts_used.append(prompt)
        if len(prompts_used) == 1:
            return bad_transcript
        return _valid_transcript()

    with (
        patch.object(runner, "_execute", side_effect=mock_execute),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.plan(issue, worker_id=0)

    assert len(prompts_used) == 2
    retry_prompt = prompts_used[1]
    assert "Validation Errors" in retry_prompt
    assert "Missing required section" in retry_prompt


@pytest.mark.asyncio
async def test_plan_gives_up_after_two_failures(config, event_bus, issue):
    """Both attempts fail — result has retry_attempted=True."""
    runner = _make_runner(config, event_bus)

    bad_transcript = "PLAN_START\nJust a one-liner.\nPLAN_END\nSUMMARY: bad"

    with (
        patch.object(runner, "_execute", AsyncMock(return_value=bad_transcript)),
        patch.object(runner, "_save_transcript"),
    ):
        result = await runner.plan(issue, worker_id=0)

    assert result.success is False
    assert result.retry_attempted is True
    assert len(result.validation_errors) > 0


@pytest.mark.asyncio
async def test_plan_no_retry_on_first_success(config, event_bus, issue):
    """Valid first attempt doesn't trigger retry."""
    runner = _make_runner(config, event_bus)

    mock_execute = AsyncMock(return_value=_valid_transcript())

    with (
        patch.object(runner, "_execute", mock_execute),
        patch.object(runner, "_save_transcript"),
    ):
        result = await runner.plan(issue, worker_id=0)

    assert result.success is True
    assert result.retry_attempted is False
    # _execute called only once
    mock_execute.assert_awaited_once()


@pytest.mark.asyncio
async def test_plan_retry_emits_retrying_status(config, event_bus, issue):
    """RETRYING status event is emitted when retrying."""
    runner = _make_runner(config, event_bus)

    bad_transcript = "PLAN_START\nJust a one-liner.\nPLAN_END\nSUMMARY: bad"

    with (
        patch.object(runner, "_execute", AsyncMock(return_value=bad_transcript)),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.plan(issue, worker_id=0)

    events = event_bus.get_history()
    planner_events = [e for e in events if e.type == EventType.PLANNER_UPDATE]
    statuses = [e.data["status"] for e in planner_events]
    assert PlannerStatus.RETRYING.value in statuses


@pytest.mark.asyncio
async def test_plan_emits_validating_status(config, event_bus, issue):
    """VALIDATING status event is emitted before validation."""
    runner = _make_runner(config, event_bus)

    with (
        patch.object(runner, "_execute", AsyncMock(return_value=_valid_transcript())),
        patch.object(runner, "_save_transcript"),
    ):
        await runner.plan(issue, worker_id=0)

    events = event_bus.get_history()
    planner_events = [e for e in events if e.type == EventType.PLANNER_UPDATE]
    statuses = [e.data["status"] for e in planner_events]
    assert PlannerStatus.VALIDATING.value in statuses


# ---------------------------------------------------------------------------
# _build_retry_prompt
# ---------------------------------------------------------------------------


def test_build_retry_prompt_includes_issue_and_errors(config, event_bus, issue):
    """Retry prompt contains issue title, failed plan, and errors."""
    runner = _make_runner(config, event_bus)
    failed_plan = "Some bad plan text"
    errors = ["Missing required section: ## Testing Strategy", "Plan has 10 words"]

    prompt = runner._build_retry_prompt(issue, failed_plan, errors)

    assert f"#{issue.number}" in prompt
    assert issue.title in prompt
    assert "Some bad plan text" in prompt
    assert "Missing required section: ## Testing Strategy" in prompt
    assert "Plan has 10 words" in prompt
    assert "PLAN_START" in prompt
    assert "PLAN_END" in prompt


# ---------------------------------------------------------------------------
# _build_prompt — schema requirements
# ---------------------------------------------------------------------------


def test_build_prompt_includes_required_schema_headers(config, event_bus, issue):
    """The updated prompt should mention all 6 required section headers."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "## Files to Modify" in prompt
    assert "## New Files" in prompt
    assert "## Implementation Steps" in prompt
    assert "## Testing Strategy" in prompt
    assert "## Acceptance Criteria" in prompt
    assert "## Key Considerations" in prompt
    assert "REQUIRED SCHEMA" in prompt


def test_build_prompt_warns_about_rejection(config, event_bus, issue):
    """The prompt should warn that plans with missing sections will be rejected."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)

    assert "rejected" in prompt.lower()


# ---------------------------------------------------------------------------
# REQUIRED_SECTIONS constant
# ---------------------------------------------------------------------------


def test_required_sections_has_six_entries(config, event_bus):
    """PlannerRunner.REQUIRED_SECTIONS should have 6 entries."""
    assert len(PlannerRunner.REQUIRED_SECTIONS) == 6
    assert "## Files to Modify" in PlannerRunner.REQUIRED_SECTIONS
    assert "## New Files" in PlannerRunner.REQUIRED_SECTIONS
    assert "## Implementation Steps" in PlannerRunner.REQUIRED_SECTIONS
    assert "## Testing Strategy" in PlannerRunner.REQUIRED_SECTIONS
    assert "## Acceptance Criteria" in PlannerRunner.REQUIRED_SECTIONS
    assert "## Key Considerations" in PlannerRunner.REQUIRED_SECTIONS


# ---------------------------------------------------------------------------
# [NEEDS CLARIFICATION] markers
# ---------------------------------------------------------------------------


def test_validate_plan_accepts_three_clarification_markers(config, event_bus):
    """Plans with 0-3 [NEEDS CLARIFICATION] markers are acceptable."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix authentication handler")
    plan = _valid_plan() + (
        "\n[NEEDS CLARIFICATION: unclear if OAuth or JWT]\n"
        "[NEEDS CLARIFICATION: which database?]\n"
        "[NEEDS CLARIFICATION: migration strategy?]\n"
    )
    errors = runner._validate_plan(issue, plan)
    assert not any("NEEDS CLARIFICATION" in e for e in errors)


def test_validate_plan_rejects_four_clarification_markers(config, event_bus):
    """Plans with 4+ [NEEDS CLARIFICATION] markers escalate."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix authentication handler")
    plan = _valid_plan() + (
        "\n[NEEDS CLARIFICATION: unclear if OAuth or JWT]\n"
        "[NEEDS CLARIFICATION: which database?]\n"
        "[NEEDS CLARIFICATION: migration strategy?]\n"
        "[NEEDS CLARIFICATION: backward compat?]\n"
    )
    errors = runner._validate_plan(issue, plan)
    assert any("NEEDS CLARIFICATION" in e for e in errors)
    assert any("4" in e for e in errors if "NEEDS CLARIFICATION" in e)


def test_validate_plan_zero_clarification_markers_ok(config, event_bus):
    """Plans with no [NEEDS CLARIFICATION] markers pass."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix authentication handler")
    errors = runner._validate_plan(issue, _valid_plan())
    assert not any("NEEDS CLARIFICATION" in e for e in errors)


# ---------------------------------------------------------------------------
# _build_prompt — [NEEDS CLARIFICATION] instruction
# ---------------------------------------------------------------------------


def test_build_prompt_includes_clarification_instruction(config, event_bus, issue):
    """Prompt should instruct the planner about [NEEDS CLARIFICATION] markers."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue)
    assert "NEEDS CLARIFICATION" in prompt


def test_build_retry_prompt_includes_clarification_instruction(
    config, event_bus, issue
):
    """Retry prompt should also mention [NEEDS CLARIFICATION] markers."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_retry_prompt(issue, "failed plan", ["some error"])
    assert "NEEDS CLARIFICATION" in prompt


# ---------------------------------------------------------------------------
# Scale detection
# ---------------------------------------------------------------------------


def test_detect_plan_scale_lite_by_label(config, event_bus):
    """Issues with a lite-plan label (e.g. 'bug') get a lite plan."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix crash", labels=["bug"])
    assert runner._detect_plan_scale(issue) == "lite"


def test_detect_plan_scale_lite_label_case_insensitive(config, event_bus):
    """Label matching is case-insensitive."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix typo", labels=["BUG"])
    assert runner._detect_plan_scale(issue) == "lite"


def test_detect_plan_scale_lite_by_short_body_and_title(config, event_bus):
    """Short body + small-fix title keyword → lite plan."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(
        number=1, title="Fix typo in README", body="Small change needed.", labels=[]
    )
    assert runner._detect_plan_scale(issue) == "lite"


def test_detect_plan_scale_full_by_default(config, event_bus):
    """Issues without lite labels or short body default to full plan."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(
        number=1,
        title="Add authentication system",
        body="A" * 600,
        labels=["feature"],
    )
    assert runner._detect_plan_scale(issue) == "full"


def test_detect_plan_scale_short_body_but_no_fix_keyword(config, event_bus):
    """Short body without small-fix keyword in title → full plan."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(
        number=1,
        title="Implement new auth system",
        body="Short body",
        labels=[],
    )
    assert runner._detect_plan_scale(issue) == "full"


def test_detect_plan_scale_custom_lite_labels(event_bus, tmp_path):
    """Custom lite_plan_labels config is respected."""
    from models import GitHubIssue

    cfg = ConfigFactory.create(
        lite_plan_labels=["hotfix", "patch"],
        repo_root=tmp_path,
    )
    runner = _make_runner(cfg, event_bus)
    issue = GitHubIssue(number=1, title="Critical fix", labels=["hotfix"])
    assert runner._detect_plan_scale(issue) == "lite"

    issue2 = GitHubIssue(number=2, title="Add authentication", labels=["bug"])
    assert runner._detect_plan_scale(issue2) == "full"


# ---------------------------------------------------------------------------
# Lite plan validation
# ---------------------------------------------------------------------------


def _lite_plan() -> str:
    """Return a plan with only lite-required sections."""
    return (
        "## Files to Modify\n\n"
        "- src/app.py — fix the crash\n\n"
        "## Implementation Steps\n\n"
        "1. Identify the root cause in app.py\n"
        "2. Apply the fix to the affected function\n"
        "3. Add error handling for edge case\n\n"
        "## Testing Strategy\n\n"
        "- Add tests/test_app.py for the crash scenario\n"
    )


def test_validate_lite_plan_accepts_three_sections(config, event_bus):
    """A lite plan with only 3 sections passes validation."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix crash")
    errors = runner._validate_plan(issue, _lite_plan(), scale="lite")
    assert errors == []


def test_validate_lite_plan_no_minimum_word_count(config, event_bus):
    """Lite plans skip the minimum word count check."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix crash")
    # _lite_plan() is well under 200 words
    errors = runner._validate_plan(issue, _lite_plan(), scale="lite")
    assert not any("words" in e for e in errors)


def test_validate_lite_plan_rejects_missing_required_section(config, event_bus):
    """Lite plan missing a required section (e.g. Testing Strategy) is rejected."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix crash")
    plan = _lite_plan().replace("## Testing Strategy", "## Tests")
    errors = runner._validate_plan(issue, plan, scale="lite")
    assert any("Testing Strategy" in e for e in errors)


def test_validate_full_plan_rejects_lite_sections_only(config, event_bus):
    """A full plan with only 3 sections fails (missing Acceptance Criteria etc.)."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Add feature")
    errors = runner._validate_plan(issue, _lite_plan(), scale="full")
    missing = [e for e in errors if "Missing required section" in e]
    assert (
        len(missing) >= 3
    )  # Missing New Files, Acceptance Criteria, Key Considerations


# ---------------------------------------------------------------------------
# Lite plan — Phase -1 gates skipped
# ---------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_lite_plan_skips_phase_minus_one_gates(config, event_bus):
    """Lite plan issues should not run Phase -1 gates."""
    from models import GitHubIssue

    runner = _make_runner(config, event_bus)
    issue = GitHubIssue(number=1, title="Fix typo", labels=["bug"])

    lite_transcript = f"PLAN_START\n{_lite_plan()}\nPLAN_END\nSUMMARY: Fix the crash"

    with (
        patch.object(runner, "_execute", AsyncMock(return_value=lite_transcript)),
        patch.object(runner, "_save_transcript"),
        patch.object(runner, "_run_phase_minus_one_gates") as mock_gates,
    ):
        result = await runner.plan(issue, worker_id=0)

    assert result.success is True
    mock_gates.assert_not_called()


# ---------------------------------------------------------------------------
# Pre-mortem prompt
# ---------------------------------------------------------------------------


def test_build_prompt_includes_pre_mortem_for_full(config, event_bus, issue):
    """Full plan prompt includes the pre-mortem section."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue, scale="full")
    assert "pre-mortem" in prompt.lower()
    assert "top 3 most likely reasons" in prompt.lower()


def test_build_prompt_no_pre_mortem_for_lite(config, event_bus, issue):
    """Lite plan prompt does NOT include the pre-mortem section."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_prompt(issue, scale="lite")
    assert "pre-mortem" not in prompt.lower()


def test_build_prompt_indicates_plan_mode(config, event_bus, issue):
    """Prompt indicates the plan mode (LITE or FULL)."""
    runner = _make_runner(config, event_bus)

    full_prompt = runner._build_prompt(issue, scale="full")
    assert "FULL" in full_prompt

    lite_prompt = runner._build_prompt(issue, scale="lite")
    assert "LITE" in lite_prompt


# ---------------------------------------------------------------------------
# Lite plan retry prompt
# ---------------------------------------------------------------------------


def test_build_retry_prompt_lite_has_fewer_sections(config, event_bus, issue):
    """Retry prompt for lite plan only lists 3 required sections."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_retry_prompt(
        issue, "failed plan", ["some error"], scale="lite"
    )
    assert "## Files to Modify" in prompt
    assert "## Implementation Steps" in prompt
    assert "## Testing Strategy" in prompt
    assert "## Acceptance Criteria" not in prompt
    assert "## Key Considerations" not in prompt


def test_build_retry_prompt_full_has_all_sections(config, event_bus, issue):
    """Retry prompt for full plan lists all 6 required sections."""
    runner = _make_runner(config, event_bus)
    prompt = runner._build_retry_prompt(
        issue, "failed plan", ["some error"], scale="full"
    )
    assert "## Files to Modify" in prompt
    assert "## Acceptance Criteria" in prompt
    assert "## Key Considerations" in prompt
