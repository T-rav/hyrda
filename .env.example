# =============================================================================
# Slack Configuration
# =============================================================================
SLACK_BOT_TOKEN=xoxb-your-bot-token-here
SLACK_APP_TOKEN=xapp-your-app-token-here
SLACK_BOT_ID=  # Optional: bot user ID (auto-detected if not provided)

# =============================================================================
# LLM Provider Configuration
# =============================================================================
LLM_PROVIDER=openai              # Options: openai, anthropic, ollama
LLM_API_KEY=your-openai-api-key-here
LLM_MODEL=gpt-4o-mini            # Or gpt-4, claude-3-haiku-20240307, etc.
LLM_BASE_URL=                    # Optional: for ollama (http://localhost:11434) or custom endpoints
LLM_TEMPERATURE=0.68
LLM_MAX_TOKENS=2000

# =============================================================================
# Embedding Configuration
# =============================================================================
EMBEDDING_PROVIDER=openai        # Options: openai, sentence-transformers
EMBEDDING_MODEL=text-embedding-3-small  # Or all-MiniLM-L6-v2 for sentence-transformers
EMBEDDING_API_KEY=               # Optional: uses LLM_API_KEY if not set
EMBEDDING_CHUNK_SIZE=1000
EMBEDDING_CHUNK_OVERLAP=200

# =============================================================================
# Vector Database Configuration (for RAG)
# =============================================================================
VECTOR_ENABLED=true
VECTOR_PROVIDER=pinecone         # Options: pinecone, elasticsearch
VECTOR_URL=http://localhost:9200 # Elasticsearch URL or Pinecone endpoint
VECTOR_API_KEY=                  # Required for Pinecone
VECTOR_COLLECTION_NAME=knowledge_base
VECTOR_ENVIRONMENT=              # Required for Pinecone (e.g., us-east-1-aws)

# =============================================================================
# RAG Configuration
# =============================================================================
RAG_MAX_CHUNKS=5
RAG_SIMILARITY_THRESHOLD=0.35
RAG_MAX_RESULTS=5
RAG_RESULTS_SIMILARITY_THRESHOLD=0.7
RAG_RERANK_ENABLED=false
RAG_INCLUDE_METADATA=true
RAG_ENABLE_HYBRID_SEARCH=false
RAG_ENABLE_CONTEXTUAL_RETRIEVAL=false  # Enable Anthropic's contextual retrieval (Pinecone only)
RAG_CONTEXTUAL_BATCH_SIZE=10           # Chunks processed in parallel for context generation

# =============================================================================
# Hybrid RAG Configuration
# =============================================================================
HYBRID_ENABLED=true              # Enable hybrid dense+sparse retrieval
HYBRID_DENSE_TOP_K=100           # Dense retrieval limit
HYBRID_SPARSE_TOP_K=200          # Sparse retrieval limit
HYBRID_FUSION_TOP_K=50           # Post-RRF fusion limit
HYBRID_FINAL_TOP_K=10            # Final results returned
HYBRID_RRF_K=60                  # RRF parameter (lower = more aggressive fusion)

# Cross-Encoder Reranking
HYBRID_RERANKER_ENABLED=true
HYBRID_RERANKER_PROVIDER=cohere  # Options: cohere
HYBRID_RERANKER_MODEL=rerank-english-v3.0
HYBRID_RERANKER_API_KEY=         # Get from cohere.ai
HYBRID_TITLE_INJECTION_ENABLED=true

# =============================================================================
# Database Configuration (for user prompts)
# =============================================================================
DATABASE_ENABLED=true
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/slack_bot

# =============================================================================
# Cache Configuration (Redis)
# =============================================================================
CACHE_ENABLED=true
CACHE_REDIS_URL=redis://localhost:6379
CACHE_CONVERSATION_TTL=1800

# =============================================================================
# Agent Processes
# =============================================================================
AGENT_ENABLED=true

# =============================================================================
# Observability (Langfuse)
# =============================================================================
LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=pk_lf_your_public_key_here
LANGFUSE_SECRET_KEY=sk_lf_your_secret_key_here
LANGFUSE_HOST=https://cloud.langfuse.com  # Or your self-hosted instance
LANGFUSE_DEBUG=false

# =============================================================================
# Application Settings
# =============================================================================
DEBUG=false
LOG_LEVEL=INFO
HEALTH_PORT=8080  # Health Check Server Port (used by health dashboard)

# =============================================================================
# Example configurations for different setups:
# =============================================================================
# === OpenAI + Elasticsearch (Recommended) ===
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-your-openai-key
# LLM_MODEL=gpt-4o-mini
# VECTOR_PROVIDER=elasticsearch
# VECTOR_URL=http://localhost:9200
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-small
# HYBRID_ENABLED=true
# # Start: docker compose -f docker-compose.elasticsearch.yml up -d

# === Anthropic + Pinecone (Cloud) ===
# LLM_PROVIDER=anthropic
# LLM_API_KEY=your-anthropic-key
# LLM_MODEL=claude-3-haiku-20240307
# VECTOR_PROVIDER=pinecone
# VECTOR_API_KEY=your-pinecone-key
# VECTOR_ENVIRONMENT=us-east-1-aws
# EMBEDDING_PROVIDER=openai
# EMBEDDING_API_KEY=your-openai-key-for-embeddings

# === Ollama (Local) + Elasticsearch ===
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://localhost:11434
# LLM_MODEL=llama2  # or any model you have pulled
# VECTOR_PROVIDER=elasticsearch
# VECTOR_URL=http://localhost:9200
# EMBEDDING_PROVIDER=sentence-transformers
# EMBEDDING_MODEL=all-MiniLM-L6-v2

# === Hybrid RAG (Pinecone + Elasticsearch) ===
# LLM_PROVIDER=openai
# LLM_API_KEY=your-openai-key
# VECTOR_ENABLED=true
# HYBRID_ENABLED=true
# VECTOR_PROVIDER=pinecone
# VECTOR_API_KEY=your-pinecone-key
# VECTOR_ENVIRONMENT=us-east-1-aws
# VECTOR_URL=http://localhost:9200  # Elasticsearch for hybrid
# HYBRID_RERANKER_ENABLED=true
# HYBRID_RERANKER_API_KEY=your-cohere-key

# === Disable RAG (Traditional Chat Mode) ===
# VECTOR_ENABLED=false
# HYBRID_ENABLED=false
# RAG_ENABLE_HYBRID_SEARCH=false
# # All other VECTOR_, EMBEDDING_, RAG_, HYBRID_ settings will be ignored
