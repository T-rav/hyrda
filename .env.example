# Slack Configuration
SLACK_BOT_TOKEN=xoxb-your-bot-token-here
SLACK_APP_TOKEN=xapp-your-app-token-here

# LLM Provider Configuration
LLM_PROVIDER=openai  # Options: openai, anthropic, ollama
LLM_API_KEY=your-openai-api-key-here
LLM_MODEL=gpt-4o-mini  # Or gpt-4, claude-3-haiku-20240307, etc.
LLM_BASE_URL=  # Optional: for ollama (http://localhost:11434) or custom endpoints
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# Vector Database Configuration (for RAG)
VECTOR_ENABLED=true
VECTOR_PROVIDER=pinecone  # Options: chroma, pinecone, elasticsearch
VECTOR_URL=http://localhost:9200  # Elasticsearch URL (for hybrid mode) or ChromaDB server
VECTOR_API_KEY=  # Required for Pinecone
VECTOR_COLLECTION_NAME=knowledge_base
VECTOR_ENVIRONMENT=us-east-1  # Required for Pinecone

# Embedding Configuration
EMBEDDING_PROVIDER=openai  # Options: openai, sentence-transformers
EMBEDDING_MODEL=text-embedding-3-small  # Or all-MiniLM-L6-v2 for sentence-transformers
EMBEDDING_API_KEY=  # Optional: uses LLM_API_KEY if not set
EMBEDDING_CHUNK_SIZE=1000
EMBEDDING_CHUNK_OVERLAP=200

# RAG Configuration
RAG_MAX_CHUNKS=5
RAG_SIMILARITY_THRESHOLD=0.7
RAG_RERANK_ENABLED=false
RAG_INCLUDE_METADATA=true

# Database Configuration (for user prompts)
DATABASE_ENABLED=true
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/slack_bot

# Cache Configuration (Redis)
CACHE_ENABLED=true
CACHE_REDIS_URL=redis://localhost:6379
CACHE_CONVERSATION_TTL=1800

# Hybrid RAG Configuration
HYBRID_ENABLED=false  # Enable hybrid dense+sparse retrieval
HYBRID_DENSE_TOP_K=100  # Dense retrieval limit (Pinecone)
HYBRID_SPARSE_TOP_K=200  # Sparse retrieval limit (Elasticsearch BM25)
HYBRID_FUSION_TOP_K=50  # Post-RRF fusion limit
HYBRID_FINAL_TOP_K=10  # Final results returned
HYBRID_RRF_K=60  # RRF parameter (lower = more aggressive fusion)

# Cross-Encoder Reranking (Optional)
HYBRID_RERANKER_ENABLED=false
HYBRID_RERANKER_PROVIDER=cohere  # Options: cohere
HYBRID_RERANKER_MODEL=rerank-english-v3.0
HYBRID_RERANKER_API_KEY=  # Get from cohere.ai

# Title Injection (Improves embeddings)
HYBRID_TITLE_INJECTION_ENABLED=true

# Agent Processes
AGENT_ENABLED=true

# Logging
LOG_LEVEL=INFO
DEBUG=false

# Health Check
HEALTH_PORT=8080

# Example configurations for different setups:

# === OpenAI + ChromaDB (Local) ===
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-your-openai-key
# LLM_MODEL=gpt-4o-mini
# VECTOR_PROVIDER=chroma
# VECTOR_URL=./chroma_db  # Local persistent storage
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-small

# === Anthropic + Pinecone (Cloud) ===
# LLM_PROVIDER=anthropic
# LLM_API_KEY=your-anthropic-key
# LLM_MODEL=claude-3-haiku-20240307
# VECTOR_PROVIDER=pinecone
# VECTOR_API_KEY=your-pinecone-key
# VECTOR_ENVIRONMENT=us-east-1
# EMBEDDING_PROVIDER=openai
# EMBEDDING_API_KEY=your-openai-key-for-embeddings

# === Ollama (Local) + ChromaDB ===
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://localhost:11434
# LLM_MODEL=llama2  # or any model you have pulled
# VECTOR_PROVIDER=chroma
# VECTOR_URL=./chroma_db
# EMBEDDING_PROVIDER=sentence-transformers
# EMBEDDING_MODEL=all-MiniLM-L6-v2

# === Hybrid RAG (Pinecone + Elasticsearch) ===
# LLM_PROVIDER=openai
# LLM_API_KEY=your-openai-key
# VECTOR_ENABLED=true
# HYBRID_ENABLED=true
# VECTOR_PROVIDER=pinecone
# VECTOR_API_KEY=your-pinecone-key
# VECTOR_ENVIRONMENT=us-east-1
# VECTOR_URL=http://localhost:9200  # Elasticsearch
# HYBRID_RERANKER_ENABLED=true
# HYBRID_RERANKER_API_KEY=your-cohere-key
# # Start Elasticsearch: docker compose -f docker-compose.elasticsearch.yml up -d

# === Disable RAG (Traditional Mode) ===
# VECTOR_ENABLED=false
# HYBRID_ENABLED=false
# # All other VECTOR_, EMBEDDING_, RAG_, HYBRID_ settings will be ignored
