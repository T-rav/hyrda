# =============================================================================
# Slack Configuration
# =============================================================================
SLACK_BOT_TOKEN=xoxb-your-bot-token-here
SLACK_APP_TOKEN=xapp-your-app-token-here
SLACK_BOT_ID=  # Optional: bot user ID (auto-detected if not provided)


# METRIC.AI
METRIC_API_KEY=?

# PORTAL (8th Light Employee Portal API)
PORTAL_SECRET=your-portal-jwt-secret-here
PORTAL_URL=https://api.portal.8thlight.com
PORTAL_EMAIL=bot@8thlight.com  # Email to use for JWT authentication

# =============================================================================
# LLM Provider Configuration
# =============================================================================
LLM_PROVIDER=openai              # Options: openai, anthropic, ollama
LLM_API_KEY=your-openai-api-key-here
LLM_MODEL=gpt-4o-mini            # Or gpt-4, claude-3-haiku-20240307, etc.
LLM_BASE_URL=                    # Optional: for ollama (http://localhost:11434) or custom endpoints
LLM_TEMPERATURE=0.68
LLM_MAX_TOKENS=2000

# =============================================================================
# Embedding Configuration
# =============================================================================
EMBEDDING_PROVIDER=openai        # Options: openai, sentence-transformers
EMBEDDING_MODEL=text-embedding-3-small  # Or all-MiniLM-L6-v2 for sentence-transformers
EMBEDDING_API_KEY=               # Optional: uses LLM_API_KEY if not set
EMBEDDING_CHUNK_SIZE=1000
EMBEDDING_CHUNK_OVERLAP=200

# NOTE: sentence-transformers is an OPTIONAL dependency (adds ~2GB PyTorch/CUDA)
# Install with: pip install -e ".[local-embeddings]"
# Default OpenAI embeddings work via API without local models

# =============================================================================
# Vector Database Configuration (Qdrant)
# =============================================================================
VECTOR_ENABLED=true
VECTOR_PROVIDER=qdrant
VECTOR_HOST=qdrant               # Qdrant service name (docker container)
VECTOR_PORT=6333                 # Qdrant REST API port
VECTOR_API_KEY=your-secure-api-key-here  # API key for Qdrant authentication
VECTOR_COLLECTION_NAME=insightmesh-knowledge-base

# =============================================================================
# RAG Configuration
# =============================================================================
RAG_MAX_CHUNKS=5
RAG_SIMILARITY_THRESHOLD=0.35
RAG_MAX_RESULTS=5
RAG_RESULTS_SIMILARITY_THRESHOLD=0.7
RAG_RERANK_ENABLED=false
RAG_INCLUDE_METADATA=true
RAG_ENABLE_HYBRID_SEARCH=false
RAG_ENABLE_QUERY_REWRITING=true        # Enable adaptive query rewriting for better retrieval
RAG_QUERY_REWRITE_MODEL=gpt-4o-mini    # Model for query rewriting (fast model recommended)

# =============================================================================
# Hybrid RAG Configuration
# =============================================================================
HYBRID_ENABLED=true              # Enable hybrid dense+sparse retrieval
HYBRID_DENSE_TOP_K=100           # Dense retrieval limit
HYBRID_SPARSE_TOP_K=200          # Sparse retrieval limit
HYBRID_FUSION_TOP_K=50           # Post-RRF fusion limit
HYBRID_FINAL_TOP_K=10            # Final results returned
HYBRID_RRF_K=60                  # RRF parameter (lower = more aggressive fusion)

# Cross-Encoder Reranking
HYBRID_RERANKER_ENABLED=true
HYBRID_RERANKER_PROVIDER=cohere  # Options: cohere
HYBRID_RERANKER_MODEL=rerank-english-v3.0
HYBRID_RERANKER_API_KEY=         # Get from cohere.ai
HYBRID_TITLE_INJECTION_ENABLED=true

# =============================================================================
# Database Configuration (for user prompts)
# =============================================================================
DATABASE_ENABLED=true
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/slack_bot

# =============================================================================
# Cache Configuration (Redis)
# =============================================================================
CACHE_ENABLED=true
CACHE_REDIS_URL=redis://localhost:6379
CACHE_CONVERSATION_TTL=1800

# =============================================================================
# Agent Processes
# =============================================================================
AGENT_ENABLED=true

# =============================================================================
# Observability (Langfuse)
# =============================================================================
LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=pk_lf_your_public_key_here
LANGFUSE_SECRET_KEY=sk_lf_your_secret_key_here
LANGFUSE_HOST=https://cloud.langfuse.com  # Or your self-hosted instance
LANGFUSE_DEBUG=false

# =============================================================================
# Prompt Management (Langfuse)
# =============================================================================
LANGFUSE_USE_PROMPT_TEMPLATES=true    # Enable Langfuse prompt templates (default: true)
LANGFUSE_SYSTEM_PROMPT_TEMPLATE=your-custom-template-name   # Set custom template name (default: insight-mesh-system-prompt)
LANGFUSE_PROMPT_TEMPLATE_VERSION=v2.0   # Optional: Use specific version (default: latest)

# =============================================================================
# MCP (Model Context Protocol) Servers
# =============================================================================
# WebCat - Web search via MCP (v2.5.0+)
MCP_WEBCAT_ENABLED=true
MCP_WEBCAT_HOST=localhost
MCP_WEBCAT_PORT=3000
MCP_WEBCAT_API_KEY=  # Optional: API key for WebCat authentication
# Serper (recommended for deep research) - Get from https://serper.dev/
MCP_SERPER_API_KEY=your-serper-api-key
# Perplexity (for deep research tool) - Get from https://www.perplexity.ai/settings/api
PERPLEXITY_API_KEY=your-perplexity-api-key
# WebCat Advanced Settings (v2.5.0+)
WEBCAT_MAX_LENGTH=10000  # Maximum content length for scraped pages (default: 10000)

# =============================================================================
# Application Settings
# =============================================================================
DEBUG=false
LOG_LEVEL=INFO
HEALTH_PORT=8080  # Health Check Server Port (used by health dashboard)
TASKS_PORT=5001   # Task Scheduler Service Port

# Convo Management Settings
CONVERSATION_MAX_MESSAGES=20              # Sliding window size
CONVERSATION_KEEP_RECENT=4                # Messages kept when summarizing
CONVERSATION_SUMMARIZE_THRESHOLD=0.75     # 75% context triggers summary
CONVERSATION_MODEL_CONTEXT_WINDOW=128000  # Model's max tokens (GPT-4o default)

# =============================================================================
# Profile Agent Configuration (Deep Research)
# =============================================================================
PDF_STYLE=professional          # Options: minimal, professional, detailed

# Token Limits for Deep Research Workflow
# Uses CONVERSATION_MODEL_CONTEXT_WINDOW as base (default: 128K)
# IMPORTANT: Compression limit must account for concurrent researchers!
# Example: 3 researchers Ã— 8K compression = 24K + overhead = fits in 128K context
RESEARCH_MODEL_MAX_TOKENS=16000       # Researcher tool calling (1/8 of context)
COMPRESSION_MODEL_MAX_TOKENS=8000     # Compression output (1/16 of context) - per researcher
FINAL_REPORT_MODEL_MAX_TOKENS=32000   # Final report output (1/4 of context) - rich, well-cited reports

# =============================================================================
# Example configurations for different setups:
# =============================================================================
# === OpenAI + Qdrant (Recommended) ===
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-your-openai-key
# LLM_MODEL=gpt-4o-mini
# VECTOR_PROVIDER=qdrant
# VECTOR_HOST=localhost
# VECTOR_PORT=6333
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-small
# # Start: docker compose up -d (Qdrant included in main compose file)

# === Anthropic + Qdrant ===
# LLM_PROVIDER=anthropic
# LLM_API_KEY=your-anthropic-key
# LLM_MODEL=claude-3-haiku-20240307
# VECTOR_PROVIDER=qdrant
# VECTOR_HOST=localhost
# VECTOR_PORT=6333
# EMBEDDING_PROVIDER=openai
# EMBEDDING_API_KEY=your-openai-key-for-embeddings

# === Ollama (Local) + Qdrant ===
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://localhost:11434
# LLM_MODEL=llama2  # or any model you have pulled
# VECTOR_PROVIDER=qdrant
# VECTOR_HOST=localhost
# VECTOR_PORT=6333
# EMBEDDING_PROVIDER=sentence-transformers
# EMBEDDING_MODEL=all-MiniLM-L6-v2
# # NOTE: Requires: pip install -e ".[local-embeddings]" for sentence-transformers

# === Disable RAG (Traditional Chat Mode) ===
# VECTOR_ENABLED=false
# # All other VECTOR_, EMBEDDING_, RAG_ settings will be ignored

# === Monitoring (Optional) ===
# Prometheus and Grafana ports for metrics and dashboards
# Run: docker compose -f docker-compose.monitoring.yml up -d
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
